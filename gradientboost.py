# -*- coding: utf-8 -*-
"""Quangtran.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oBrljj5GRY11n4YiS_K7eTkwQPpMWxBN
"""

from google.colab import drive

drive.mount('/content/drive')

!ls '/content/drive/MyDrive/Data 270 project/Data'

import pandas as pd

df=pd.read_csv(r'/content/drive/MyDrive/Data 270 project/Data/NORMALIZED_TABLE.csv')

df.columns

#Settings
import pandas as pd
import numpy as np
from sklearn import ensemble
from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error, explained_variance_score, r2_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, KFold
from sklearn.model_selection import RepeatedKFold

columns=[ 'PRCPUSC00045118', 'PRCPUSC00045933', 'PRCPUSC00046168',
       'PRCPUSC00049473', 'PRCPUSW00023233', 'PRCPUSW00093243',
       'TAVGUSC00045118', 'TAVGUSC00045933', 'TAVGUSC00046168',
       'TAVGUSC00049473', 'TAVGUSW00023233', 'TAVGUSW00093243',
       'TMAXUSC00045118', 'TMAXUSC00045933', 'TMAXUSC00046168',
       'TMAXUSC00049473', 'TMAXUSW00023233', 'TMAXUSW00093243',
       'TMINUSC00045118', 'TMINUSC00045933', 'TMINUSC00046168',
       'TMINUSC00049473', 'TMINUSW00023233', 'TMINUSW00093243', 'SUM_SWE',
       'storage']

df2=df[columns]
df1=df[['SUM_SWE','storage']]

#ML

X,y=df2.drop('storage',axis=1),df2['storage']

#Test
X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=.2, shuffle=True, random_state=13)


params = {
    "n_estimators": 800,
    "max_depth": 5,
    "min_samples_split": 5,
    "learning_rate": 0.05,
    "loss": "squared_error",
}

#Create ML model
reg = ensemble.GradientBoostingRegressor(verbose=0,**params)
reg.fit(X_train, y_train)

#Predict
pred=reg.predict(X_test)

#scores
rscore=r2_score(y_test,pred)
mse = mean_squared_error(y_test, reg.predict(X_test))
rmse = np.sqrt(mean_squared_error(y_test, reg.predict(X_test)))
evs= explained_variance_score(y_test,reg.predict(X_test))

#validation K-fold
kfold=KFold(n_splits=10, shuffle=True)
#kf_cv_score=cross_val_score(reg, X_train, y_train,cv=kfold)
#kf_cv_score

Tests=[]
Train=[]
R2=[]
for i in range(100,1000,100):
  print(i)
  params = {
      "n_estimators": i,
      "max_depth": 5,
      "min_samples_split": 5,
      "learning_rate": 0.05,
      "loss": "squared_error",
  }
  reg = ensemble.GradientBoostingRegressor(verbose=0,**params)
  reg.fit(X_train, y_train)
  Tests.append(round(reg.score(X_test,y_test),2))
  Train.append(round(reg.score(X_train,y_train),2))
  R2.append(round(r2_score(y_test,pred),2))
x_ax=list(range(100,1000,100))
plt.figure(figsize=(13,8.5))
plt.plot(x_ax,Tests,label="Test score")
plt.plot(x_ax, Train,label= "Train score")
plt.plot(x_ax,R2, label='R2 score')
plt.xlabel("n_estimators")
plt.legend()
plt.vlines(x_ax,0,1,linestyles='dashed', colors='black')

#Score card
print(f'Train score {reg.score(X_train,y_train)}')
print(f'Test score {reg.score(X_test,y_test):.2f}')
print(f'R2 score is {rscore}')
print(f"The mean squared error (MSE) on test set: {mse}")
print(f"The root mean squared error (RMSE) on test set: {rmse}")
#print("The explained variance score on test set: {:.2f}".format(evs))
#print(f'The K-Fold cross-validation average score is {kf_cv_score.mean():.2f}')

plt.figure(figsize=(11,8.5))
pred=reg.predict(X_test[:100])

x_ax=range(len(y_test[:100]))
plt.scatter(x_ax, y_test[:100], label="Labeled/Original Data", color='blue')
plt.plot(x_ax,pred, label="Prediction", color='orange')
plt.title("Labeled vs Predicted Reservoir Level")
plt.xlabel("Input Data")
plt.ylabel("Resulting Output")
plt.xticks(x_ax,color='w')
plt.legend()
#plt.vlines(x_ax,0,1,linestyles='dashed', colors='black')

plt.show()

X_test[:100]

y_test[:100]

